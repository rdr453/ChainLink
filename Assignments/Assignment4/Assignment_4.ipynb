{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdr453/ChainLink/blob/main/Assignments/Assignment4/Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4: Variational Autoencoder for Face Image Synthesis\n",
        "\n",
        "\n",
        "## Student Name: Bobby Rizzo\n",
        "\n",
        "\n",
        "Objective: The goal of this assignment is to implement a Variational Autoencoder (VAE) using a deep neural network and train it on the CelebA dataset to generate synthetic face images. This exercise will provide hands-on experience with building and training a probabilistic generative model.\n",
        "\n",
        "Random CelebA inhabitant: David Niven (1910-1983). One Academy Award and two Golden Globes for movies released about 70 years ago.\n",
        "\n",
        "<img src=https://www.cs.toronto.edu/~lindell/teaching/420/slides/celebahq/37.jpg width=128 height=128>\n"
      ],
      "metadata": {
        "id": "JaPE8SG5MGUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put all of your `import`s in the cell below."
      ],
      "metadata": {
        "id": "R2TZi5ZetUra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import all of the bits\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import random\n",
        "import math\n",
        "import tqdm.notebook as tqdm\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import gdown\n",
        "import PIL\n",
        "import datetime\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as tt\n",
        "import torchvision.datasets as td\n",
        "\n",
        "import torchsummary\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# some housekeeping; random stuff & cuda\n",
        "_ = torch.random.seed()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Executing NN code on {device}')"
      ],
      "metadata": {
        "id": "4OC6uDYEQiRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Freebie: Dataset Preparation\n"
      ],
      "metadata": {
        "id": "AruXadNrMWXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're getting code to prep the CelebA dataset for training use.  The CelebA images are all 218 rows x 178 columns x 3 channels.\n",
        "\n",
        "The steps performed below are:\n",
        " 1. Copy the zip file containing CelebA from my Google drive location to your runtime. Recycling some earlier code from Assignment 2.\n",
        " 2. Create a `Dataset` for training (call it `celeba_train_ds`) , specifying the CelebA training partition. We don't care about the labels, because the \"target\" when training is the input itself.  Specify `torchvision.transforms.ToTensor()` as the transform -- that will take care of converting the PIL images in the zipfile to PyTorch tensors, with each component scaled to [0,1].\n",
        " 3. Repeat step 2 for the validation partition. Call the `Dataset` by the name `celeba_val_ds`.\n",
        " 4. As a sanity check, the code renders a random image from `celeba_train_ds` and a random image from `celeba_val_ds` using Matplotlib's `imshow()` method.  You can use subscripting to get the first sample from a data set, which returns a (data,target) tuple. Subscript that to get the data. Permute the dimensions of the data using `torch.permute()` to put the channels last, and send the result to `plt.imshow()`  In the depicted images, see how the eyes are in approximately the same positions in each image.  The CelebA data is _registered_, which means the faces are scaled and translated as needed to put the facial features in approximately the same location in all images."
      ],
      "metadata": {
        "id": "uyP5QbHgsLMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FREEBIE CODE: DATA\n",
        "\n",
        "DATA_ROOT_DIR = \"celeba\"\n",
        "\n",
        "if not os.path.exists(DATA_ROOT_DIR):\n",
        "    print('Downloading ND copy of celeba.zip - this will take a minute or so.')\n",
        "    gdown.download('https://drive.google.com/uc?id=1gyR8WSFURkc4TWil9zik3OP03Wj4D4bz','celeba.zip')\n",
        "    print('Unzipping celeba.zip - this will take a few minutes.')\n",
        "    with zipfile.ZipFile('celeba.zip', 'r') as z:\n",
        "        z.extractall('.')\n",
        "    print('Done!')\n",
        "\n",
        "SIZE = 160\n",
        "\n",
        "transform = tt.Compose([tt.ToTensor(),tt.CenterCrop(SIZE)])\n",
        "\n",
        "celeba_train_ds = td.CelebA(\".\",split=\"train\", transform=transform,download='False')\n",
        "celeba_val_ds = td.CelebA(\".\",split=\"valid\", transform=transform, download='False')\n",
        "\n",
        "tmp_train = celeba_train_ds[0][0]\n",
        "print(f'{tmp_train.shape}')\n",
        "tmp_train = torch.permute(tmp_train,[1,2,0])\n",
        "\n",
        "tmp_val = celeba_val_ds[0][0]\n",
        "tmp_val = torch.permute(tmp_val,[1,2,0])\n",
        "\n",
        "fig,(ax1,ax2) = plt.subplots(1,2)\n",
        "ax1.imshow(tmp_train)\n",
        "ax2.imshow(tmp_val)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ye-ZFaCEQwLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. VAE Model Implementation\n",
        "\n",
        "A VAE consists of:\n",
        " * an encoder that reduces the image to a _latent vector_ whose dimensionality you get to explore\n",
        " * a decoder that takes a latent vector and creates an image from it.\n",
        "\n",
        "You'll implement an `Encoder` class and a `Decoder` class, as well as a `VAE` class which uses both of them.\n",
        "\n",
        "When designing the VAE (as with most nets that use convolutions), the size of the input doesn't matter in the convolutional layers, but it does matter in the fully connected layers that are attached to those convolutional layers. Since the CelebA dataset images are fixed in size, and it's a pretty reasonable size, we're going to avoid making the network adapt to different image sizes."
      ],
      "metadata": {
        "id": "sjcvyU9jNL7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2a. Encoder Network\n",
        "Design and implement a convolutional neural network (CNN) to act as the encoder. The encoder will take a face image as input and output the parameters (mean $\\mu$ and log variance $\\log \\sigma^2$) of a lower-dimensional latent Gaussian distribution $q(z|x, \\theta)$ for the input image $x$, with encoder parameters $\\theta$.\n",
        "\n",
        "Your encoder class should be named `Encoder`, and should consist of the blocks listed below. Your implementation may split the functionality in these blocks between `__init__()` and `forward()`; the exact split is up to you.\n",
        "\n",
        "* Your `__init__()` method must take two parameters:\n",
        " * `in_channels`, number of input channels, with a default of 3.\n",
        " * `latent_dim`, the dimensionality of the output latent spaces for $\\mu$ and $\\log \\sigma^2$, with a default of 128.\n",
        "\n",
        "* Your `forward()` method will receive a batch of images. This batch will be a `Tensor` of size `(bs, 3, 160,160)`, where `bs` is the batch size of your `DataLoader`. You will perform a forward pass with the batch, and output a tuple of two tensors:\n",
        " * `mu`, which will contain the mean of a probabilistic (multivariate Gaussian) model for the data, and\n",
        " * `logvar`, which will contain the natural logarithms of the variances of the components of the latent space.\n",
        "Both `mu` and `logvar` will have shape `(bs,latent_dim)`\n",
        "\n",
        "**Blocks:**\n",
        " * block 1:\n",
        "  * a `Conv2d` with a 4x4 kernel, a `stride` of 2, a `padding` of 1, and 32 output channels.  It will transform the `(bs,3,160,160)` input to a `(bs,32,80,80)` output.\n",
        "  * `BatchNorm2d`\n",
        "  * `ReLU()`\n",
        " * block 2:\n",
        "  * a `Conv2d` with a 4x4 kernel, a `stride` of 2, a `padding` of 1, and 64 output channels. `(bs,32,80,80)` --> `(bs,64,40,40)`\n",
        "  * `BatchNorm2d`\n",
        "  * `ReLU()`\n",
        " * block 3:\n",
        "  * a `Conv2d` with a 4x4 kernel, a `stride` of 2, a `padding` of 1, and 128 output channels. `(bs, 64, 40, 40)` --> `(bs,128,20,20)`\n",
        "  * `BatchNorm2d`\n",
        "  * `ReLU()`\n",
        " * block 4:\n",
        "  * a `Conv2d` with a 4x4 kernel, a `stride` of 2, a `padding` of 1, and 128 output channels. `(bs,128,20,20)` --> `(bs,256,10,10)`\n",
        "  * `BatchNorm2d`\n",
        "  * `ReLU()`\n",
        " * block 5:\n",
        "  * a `Conv2d` with a 4x4 kernel, a `stride` of 2, a `padding` of 1, and 128 output channels. `(bs,256,10,10)` --> `(bs,512,5,5)`\n",
        "  * `BatchNorm2d`\n",
        "  * `ReLU()`\n",
        " * block 6:\n",
        "  * a `view` to unravel the output of block 3 (of size `(bs,512,5,5)`) to a batch of 1d vectors of size `(bs,512*5*5)`\n",
        "  * The result of the `view` operator will be provided to two layers _in parallel_:\n",
        "     * a `Linear` layer `fc_mu` with `512*5*5` inputs and `latent_dim` outputs that outputs the estimated mean.\n",
        "     * another `Linear` layer `fc_logvar` with `512*5*5` inputs and `latent_dim` outputs that outputs the log of the estimated variance.\n",
        "\n",
        "    The figure below illustrates these two parallel units.\n",
        "\n",
        "**Note**: the implementation of the `Encoder` should be _much, much_ shorter than the explanation above. 😀"
      ],
      "metadata": {
        "id": "r5CwljbLsbyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE IS FILLED IN BELOW\n",
        "\n",
        "# Encoder Definition\n",
        "# input is (-1,3,160,160)\n",
        "class Encoder(torch.nn.Module):\n",
        "    \"\"\"The encoder of an autoencoder.\"\"\"\n",
        "    def __init__(self, in_channels=3, latent_dim=128):\n",
        "        \"\"\" set up encoder.\n",
        "        in_channels is number of input channels (e.g., 3 for color, 1 for grayscale).\n",
        "        latent_dim is the size of each vector in the elements of the output tuple.\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # set up layers, etc.\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" generate output from input.\n",
        "        input size (-1, 3, 160, 160).\n",
        "        output is a tuple with sizes ((-1,latent_dim), (-1,latent_dim)).\n",
        "        \"\"\"\n",
        "        # execute layers\n",
        "        # ...\n",
        "        # return a tuple of two batches of `latent_dim` sized vectors\n",
        "        return mu, logvar"
      ],
      "metadata": {
        "id": "fmrZwoEsOS40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2b. Decoder Network\n",
        "\n",
        "The second component of the VAE implementation is a decoder class, to be named `Decoder`. The decoder will take a sample $z$ from the latent space as input and output a reconstructed face image $y = g(z, \\phi)$, where $\\phi$ are the decoder parameters. The decoder should have a structure that is roughly the inverse of the encoder, using transposed convolutional layers, batch normalization, and activation functions. The final layer should output an image with the same dimensions and number of channels as the input images.\n",
        "\n",
        "The inputs to the `Decoder` constructor are\n",
        " * `latent_dim`, default 128\n",
        " * `output_channels`, default 3.\n",
        "\n",
        "The `forward` method's input is a batch of latent vectors, with size `(bs,latent_dim)`.\n",
        "It produces an output of shape `(bs,output_channels,160,160)`\n",
        "\n",
        "[_Note_: You might say \"Whaaat? The encoder creates a _tuple_ with **each element** of size (batch size,`latent_dim`). How can that be coerced into a batch of `latent_dim`-dimensional vectors that the decoder needs?\"  Good for you for noticing this seeming problem. The answer (at least the practical part) is below - it's called the _reparameterization trick_. For now, just assume that each latent vector going in is `latent_dim`-dimensional. ]\n",
        "\n",
        "Given this latent input, the layers are:\n",
        " * a `Linear` layer that maps the latent vector batch `(bs,latent_dim)` into an output batch of size `(bs,5*5*512)`.\n",
        " * a `view` that converts the output batch above to size `(bs,512,5,5)`\n",
        " * a [`ConvTranspose2d`](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) layer with `kernel_size`=4, `stride`=2, and 256 outputs  (fractional stride will double the size of the output horizontally and\n",
        " vertically, so the shape change is `(bs,512,5,5)` --> `(bs,256,10,10)`).\n",
        " * ReLU\n",
        " * a `ConvTranspose2d` layer with `kernel_size`=4, `stride`=2, and 128 outputs. `(bs,256,10,10)` --> `(bs,128,20,20)`.\n",
        " * ReLU\n",
        " * a `ConvTranspose2d` layer with `kernel_size`=4, `stride`=2, and 64 outputs. `(bs,128,20,20)` --> `(bs,64,40,40)`.\n",
        " * ReLU\n",
        " * a `ConvTranspose2d` layer with `kernel_size`=4, `stride`=2, and 32 outputs. `(bs,64,40,40)` --> `(bs,32,80,80)`.\n",
        " * ReLU\n",
        " * a `ConvTranspose2d` layer with `kernel_size`=4, `stride`=2, and `output_channels` outputs. `(bs,32,80,80)` --> `(bs,output_channels,160,160)`.\n",
        " * (**no `ReLU` here!**)\n",
        " * Sigmoid\n",
        "\n",
        "The output of the decoder is the output of the sigmoid."
      ],
      "metadata": {
        "id": "txYTjsG9ORpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE BELOW\n",
        "\n",
        "# Decoder Definition\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, latent_dim=128, out_channels=3):\n",
        "        super(Decoder, self).__init__()\n",
        "        # specify the layers.\n",
        "\n",
        "    def forward(self, z):\n",
        "        # implement the forward pass\n",
        "        #\n",
        "        # ...\n",
        "        #\n",
        "        return reconstruction"
      ],
      "metadata": {
        "id": "y3uyO1kAOeGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2c. The VAE\n",
        "\n",
        "Tie the encoder and decoder together into a VAE class. Call it `VAE`. Its constructor should take two parameters: `input_channels` which defaults to 3, and `latent_dim` (the dimensionality of the latent space) which defaults to 128. `input_channels` and `latent_dim` are supplied to the constructor for `Encoder`; `latent_dim` and `input_channels` are supplied to the constructor for `Decoder`; thus, the output batch should have the same size as the input batch.\n",
        "\n",
        "The `__init__()` method should instantiate an Encoder and a Decoder with proper parameters and store them in the `VAE` instance; _e.g., `self.Encoder = Encoder( ... )`\n",
        "\n",
        "The `forward()` method will:\n",
        " * take a batch of input images `x` (shape `(bs,input_channels,160,160)` as provided by the `DataLoader` you'll create shortly),\n",
        " * encode the batch (getting a tuple (`mu`,`logvar`)),\n",
        " * _reparametrize the tuple into a batch of latent vectors_ `z` (see below),\n",
        " * and provide `z` to the decoder (getting a batch of output images which will reconstruct `x`).\n",
        " That reconstructed batch, and `mu` and `logvar` as supplied by the decoder, are the output of `forward()`.  You have to output `mu` and `logvar` because they're needed by the loss function.\n",
        "\n",
        "#### Latent Space Sampling ([Reparameterization Trick](https://en.wikipedia.org/wiki/Reparameterization_trick))\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/1/11/Reparameterized_Variational_Autoencoder.png)\n",
        "Implement the reparameterization trick to enable backpropagation through the stochastic sampling process. This involves sampling a latent vector $z$ from the approximate posterior distribution $q(z|x, \\theta)$ using the formula $z = \\mu + \\sigma \\epsilon$, where $\\epsilon$ is a sample from a standard normal distribution $N(0, I)$ and $\\sigma = \\exp(\\frac{1}{2} \\log \\sigma^2)$.\n",
        "\n",
        "To explain in a more [Torchy](https://pytorch.org) way ... implement a method `reparametrize` that takes `mu` and `logvar` as parameters, and does this:\n",
        " * compute standard deviation $\\sigma$ from log variance (use `torch.exp()`): `std` = $e^{\\mbox{logvar/2}}$\n",
        " * compute a normal deviate with zero mean and standard deviation `std` (use `std * torch.randn_like(std)` - the second use of `std` sets the size and the first use scales the output)\n",
        " * add `mu` to the scaled output of `torch.randn_like()`. That's the output of the method."
      ],
      "metadata": {
        "id": "3efyjI54OoKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE\n",
        "\n",
        "# VAE Model Definition\n",
        "class VAE(torch.nn.Module):\n",
        "    def __init__(self, input_channels=3, latent_dim=128):\n",
        "        super(VAE, self).__init__()\n",
        "        # define the pieces\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        # return a batch the same shape as logvar or mu\n",
        "        # it contains uniform random deviates scaled by exp(logvar/2)\n",
        "        # and offset by mu\n",
        "        return  .... something ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        # define the forward operation\n",
        "        # ...\n",
        "        # ...\n",
        "        return reconstruction, mu, logvar"
      ],
      "metadata": {
        "id": "LOPyoXXBOuF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3. Loss Function\n"
      ],
      "metadata": {
        "id": "6CpVevXLOc2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total loss function to be minimized in a VAE is the negative [Evidence Lower Bound (ELBO)](https://en.wikipedia.org/wiki/Evidence_lower_bound):\n",
        "\\begin{align}\n",
        "L(\\theta, \\phi) = -E_{q(z|x, \\theta)}[\\log p(x|z, \\phi)] + D_{KL}(q(z|x, \\theta) || p(z))\n",
        "\\end{align}\n",
        "\n",
        "The first term corresponds to the reconstruction loss, and the second to the Kullback-Liebler divergence.\n",
        "\n",
        "Implement the VAE loss function, which consists of two terms:\n",
        " * Reconstruction Loss: This term measures how well the decoder reconstructs the input image from the latent representation. Use `torch.functional.mse_loss()` with `reduction=sum` for this.\n",
        "\n",
        "* [Kullback-Liebler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence): This term measures the difference between the approximate posterior distribution $q(z|x, \\theta) = N(\\mu, \\sigma^2 I)$ and the prior distribution $p(z)$, which is typically a standard normal distribution $N(0, I)$. The KL divergence between two Gaussian distributions $N(\\mu_1, \\sigma_1^2 I)$ and $N(0, I)$ has a closed-form expression:\n",
        "\\begin{align}\n",
        "D_{KL}(N(\\mu_1, \\sigma_1^2 I) || N(0, I)) = \\frac{1}{2} \\sum_{j=1}^{D_z} (1 + \\log(\\sigma_{1j}^2) - \\mu_{1j}^2 - \\sigma_{1j}^2),\n",
        "\\end{align}\n",
        "where $D_z$ is the dimensionality of the latent space.\n",
        "\n",
        "To clarify something about this loss that's not obvious in the first equation in this cell: the loss function is MSE **minus** the KL Divergence.\n",
        "\n",
        "In practice, we generally create a hyperparameter that allows the contributions of MSE and KL Divergence to be traded off against one another. We'll call this hyperparameter `alpha`, and it will be a multiplier on the  KL Divergence.\n",
        "\n",
        "\n",
        "Thus: Implement a function `loss_vae(xpred,x,mu,logvar, alpha)` that computes and returns loss values as MSE - `alpha` * KL Divergence.\n",
        "\n",
        "Note: although PyTorch provides a function called `KLDivLoss()`, you shouldn't use it here. Implement the second equation above, using `torch.sum()`, and the `.pow()` and `.exp()` methods of Torch tensors."
      ],
      "metadata": {
        "id": "7_lxTjAVsfUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#STUDENT CODE\n",
        "\n",
        "# Loss Function\n",
        "def vae_loss(reconstruction, x, mu, logvar, alpha):\n",
        "    \"\"\"compute VAE loss as a weighted sum of MSE and KL Divergence.\"\"\"\n",
        "    # Reconstruction loss (e.g., MSE)\n",
        "    recon_loss = F.mse_loss(reconstruction, x)\n",
        "\n",
        "    # KL divergence loss\n",
        "    # (implement it)\n",
        "    # ...\n",
        "\n",
        "    return recon_loss - alpha *  ... something ..."
      ],
      "metadata": {
        "id": "IsH-xtKFPAmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4. Training\n",
        "\n",
        "Implement training in three cells of code below."
      ],
      "metadata": {
        "id": "-REfxd7_PADi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4a. Function: one iteration of training\n",
        "\n",
        "Define a function  `train_epoch(epoch,model,loader,optimizer,loss_fn,alpha)` that performs one epoch of training of the specified `model`, whose training `DataLoader` is `loader`, using the supplied `optimizer`, and the supplied `loss_fn` which requires the supplied parameter `alpha`.`epoch` is the epoch index. Make sure the net is in `train()` mode when you are doing the training. Ths function should return the training loss for the epoch."
      ],
      "metadata": {
        "id": "xkkTg3Gmsh7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE\n",
        "\n",
        "def train_epoch(epoch,model,loader,optimizer,loss_fn,alpha):\n",
        "    \"\"\"train a single epoch.\n",
        "    Note: not generic, because it assumes VAE output (prediction,mu,logvar)\n",
        "    and calls loss_fn accordingly\"\"\"\n",
        "\n",
        "    # implement the loop\n",
        "\n",
        "    return ... the loss you calculated ..."
      ],
      "metadata": {
        "id": "tGYza2qlikUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4b. Training with specified hyperparameters\n",
        "Define a function `train(model,optimizer,latent_dim,batch_size,lr,epochs,alpha)` that trains and saves a model as guided by the optimizer and other parameters.\n",
        "\n",
        "In your implementation of the function before training commences, choose a random image from the `celeba_train_ds` `Dataset`. Use `torch.permute()` and `torch.unsqueeze()` to turn that image (original shape `(160,160,3)` into a 1-sample batch of shape `(1,3,160,160)`.  We'll call this the \"test image\".\n",
        "\n",
        "You'll need to instantiate the model and then create an Adam optimizer for it, providing it with the model parameters and the learning rate.\n",
        "\n",
        "Your function should **use the VALIDATION partition of `CelebA` for training** (it's smaller and will train the net way faster than the training partition). Build a `DataLoader` for that dataset using the specified batch_size and supply it to the `train_epoch()` function. After each epoch of training, print out the loss for that epoch. Also, at the end of the epoch, switch the net to `eval()` mode, and pass the test image through the network. Use `squeeze()` and `permute()` to render the reconstructed output side-by-side with the input test image so you can track improvement in reconstruction. Compute and print the reconstruction loss using the `vae_loss()` function.\n",
        "\n",
        "At the end of the cell, use `torch.save()` to save the model parameters using a unique name ending in `.pth`. More info on loading and saving models is [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference).  You should keep track of the hyperparameter settings associated with each saved model.\n",
        "\n",
        "Make sure you download the `.pth` file to your local computer if you want to save it, as it will be removed when your Colab session is shut down."
      ],
      "metadata": {
        "id": "vKlz1pp_q5nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE\n",
        "\n",
        "def train(model,latent_dim,batch_size,lr,optimizer,epochs,alpha):\n",
        "    \"\"\"run training with specified model, optimizer, and parameters .\n",
        "    \"\"\"\n",
        "    celeba_train_dl = torch.utils.data.DataLoader(celeba_train_ds,batch_size=batch_size)\n",
        "    celeba_val_dl = torch.utils.data.DataLoader(celeba_val_ds,batch_size=batch_size)\n",
        "\n",
        "    # stage a randomly chosen sample image from the training partition on 'device'\n",
        "    test_img_tmp = random.choice(celeba_train_ds)[0] # (3,SIZE,SIZE)\n",
        "    test_img = torch.unsqueeze(test_img_tmp,0) # (1,3,SIZE,SIZE)\n",
        "    test_img = test_img.to(device) # --> GPU if we have one\n",
        "    test_img_perm = torch.permute(test_img_tmp,[1,2,0]) # (SIZE,SIZE,3) for display\n",
        "\n",
        "    # show sample image\n",
        "    print(\"I'll use this sample image while training.\")\n",
        "    _ = plt.imshow(test_img_perm)\n",
        "    plt.show()\n",
        "\n",
        "    for epoch in tqdm.trange(epochs):\n",
        "        # ... inner part of training loop ...\n",
        "\n",
        "    # save model\n",
        "    # ...\n"
      ],
      "metadata": {
        "id": "Pl-UI_duedLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5. Experiments\n"
      ],
      "metadata": {
        "id": "SNp9HthXPZHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5a. _Ad hoc_ hyperparameter search\n",
        "\n",
        "Consider this set of hyperparameter values:\n",
        " * `lr` (learning rate): 0.1, 0.01, 0.001\n",
        " * `alpha`: 0.1, 0.01, 0.001\n",
        " * `latent_dim`: 64, 128, 256\n",
        "\n",
        "Perform a hyperparameter search by training the VAE net for 20 epochs over four different combinations of these hyperparameters, as specified by student in the list below.  Since 4x7 is 28, one combination will be done by two students. Use a batch size of 128 samples.\n",
        "\n",
        "* Matt (0.1, 0.1, 64) (0.1, 0.001, 128) (0.01, 0.01, 256) (0.001, 0.01, 64)\n",
        "* Will (0.1, 0.1, 128)\n",
        "(0.1, 0.001, 256)\n",
        "(0.01, 0.001, 64)\n",
        "(0.001, 0.01, 128)\n",
        "* Joe  (0.1, 0.1, 256)\n",
        " (0.01, 0.1, 64)\n",
        " (0.01, 0.001, 128)\n",
        "(0.001, 0.01, 256)\n",
        "* Samara(0.001, 0.001, 64)\n",
        " (0.1, 0.01, 64)\n",
        "(0.01, 0.1, 128)\n",
        " (0.01, 0.001, 256)\n",
        "* Ryan (0.1, 0.01, 128)\n",
        " (0.001, 0.1, 64)\n",
        " (0.01, 0.1, 256)\n",
        "(0.001, 0.001, 128)\n",
        "* Bobby (0.1, 0.01, 256)\n",
        " (0.01, 0.01, 64)\n",
        " (0.001, 0.1, 128)\n",
        "(0.001, 0.001, 256)\n",
        "* Brisny (0.1, 0.001, 64)\n",
        " (0.01, 0.01, 128)\n",
        "(0.001, 0.1, 256)\n",
        " (0.001, 0.1, 64)\n",
        "\n",
        "In the text cell below, describe the results of your four tests. Report, for each test:\n",
        " * the hyperparameter values used\n",
        " * the training loss\n",
        " * your opinion of the visual quality of the results.  Rank the four sample output images by subjective visual quality (it's fine if you use a different test image for each combination of hyperparameter values)."
      ],
      "metadata": {
        "id": "Wae1lEilgUCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE GOES HERE\n",
        "\n"
      ],
      "metadata": {
        "id": "IwZ_22vr2lMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STUDENT RESPONSE GOES HERE"
      ],
      "metadata": {
        "id": "lzqk_CvkgKM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5b. Reconstruction from random latent vectors\n",
        "\n",
        "In the code cell below\n",
        "* Use `torch.load()` to load the best-performing VAE from your experiments above.\n",
        "* Create a batch of 16 latent vectors $z$ from the prior distribution $p(z) = N(0, I)$.  Use [`torch.randn()`](https://pytorch.org/docs/stable/generated/torch.randn.html) with a suitable size specification.\n",
        "\n",
        "* Pass the batch of latent vectors through the VAE Decoder stage to get 16 output images.\n",
        "* Display the synthesized images -- ideally  in a 4x4 grid of images (use `plt.subfigure()` to set up the grid).\n",
        "* Comment on the visual quality and diversity (gender, skin tone, etc.) of the generated images. The results of any VAE trained on CelebA will be biased toward Causasian skin. Why?\n"
      ],
      "metadata": {
        "id": "sfv7qffYsjzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE\n",
        "\n",
        "PATH = '/content/name_of_pytorch_save_file.pth'\n",
        "\n",
        "# ... load model and put it in train mode\n",
        "\n",
        "NUM_IMAGES = 16\n",
        "\n",
        "# Sampling from latent space. We're not starting with an image, so\n",
        "# we can't assess the loss numerically - but we can characterize quality\n",
        "# visually\n",
        "with torch.no_grad():\n",
        "    # create random latent vectors from N(0,I)\n",
        "    # then feed them to the decoder, getting a result\n",
        "    z_batch = torch.randn(NUM_IMAGES, 64).to(device)\n",
        "    generated_image_batch = vae.decoder(z_batch)\n",
        "\n",
        "# outside the no_grad scope...\n",
        "# move images to cpu\n",
        "#\n",
        "# set up a 4x4 grid of subplots\n",
        "# loop over the generated image batch\n",
        "#   place that image in the proper cell in the grid of subplots\n"
      ],
      "metadata": {
        "id": "lAL6CGK1PmBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STUDENT COMMENTARY RESPONSE HERE"
      ],
      "metadata": {
        "id": "GtdcYjKQPjJa"
      }
    }
  ]
}